{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретические задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача баесовского классификатора:\n",
    "$$P(y|x^1....x^N) = P(y) \\prod_{k = 1}^N P(x^k|y)$$\n",
    "Где мы ищем максимальный $P(y|x^1....x^N)$ по всем классам $y$.\n",
    "По условию задачи условные вероятности имеют вид:\n",
    "$$P(x^{(k)}|y) = \\frac{1}{\\sqrt{2\\Pi\\sigma^2}} e^{-\\frac{(x^{(k)} - \\mu_{yk})^2}{2\\sigma^2}}$$\n",
    "Подставим их в $P(y|x^1....x^N)$:\n",
    "\n",
    "$$P(y|x^1....x^N) =  C P(y) \\prod_{k = 1}^N e^{-\\frac{(x^{(k)} - \\mu_{yk})^2}{2\\sigma^2}} = C P(y) e^{-\\frac{\\sum_{k=1}^N(x^{(k)} - \\mu_{yk})^2}{2\\sigma^2}}$$\n",
    "Где $C$ - некоторая константа, одинаковая для всех классов. Видно, что $P(y|x^1....x^N)$ максимально тогда, когда максимальна степень экспоненты, т.е. минимальна $\\sum_{k=1}^N(x^{(k)} - \\mu_{yk})^2$, что и есть евклидово расстояние до центра класса $\\mu_y$.\n",
    "Ч.т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Треугольный roc-auc будет иметь три точки для трёх возможных порогов: ниже 0, между 0 и 1 и при пороге выше 1. Первый и последний пороги будут давать точки (0,0) и (1,1). В задаче вероятность, с которой мы предсказываем класс 1, равна $p$. Пусть доля классов в выборке равна $u$.\n",
    "Найдём координаты точки для среднего порога:\n",
    "\n",
    "Найдём $TPR$. По определению $TPR = \\frac{TP}{TP + FN} = \\frac{pu}{u} = p$, т.к. вероятность предсказать правильно класс 1 для случайного элемента равна $pu$.\n",
    "\n",
    "Найдём $FPR$. $FPR =\\frac{FP}{FP + TN} =\\frac{p(1 - u)}{(1 - u)} = p$\n",
    "\n",
    "Значит эта точка всегда будет иметь координаты $(p , p)$, которые лежат на прямой $y=x$, а значит ломаная, проведённая через эти три точки фактически является прямой(в случае, если выборка бесконечно большая). Ну а площадь такого треугольника как раз и есть $\\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выпишем вероятности ошибки баесовского и 1NN классификаторов\n",
    "$$E_B = min(P(1\\ |\\ x), P(0\\ |\\ x))$$\n",
    "$$E_N = P(y\\neq y_n) = P(1\\ |\\ x) P(0\\ |\\ x_n) + P(0\\ |\\ x)P(1\\ |\\ x_n) \\leq P(1\\ | \\ x) + P(0\\ | \\ x) \\leq 2 * min(P(1\\ |\\ x), P(0\\ |\\ x))$$\n",
    "\n",
    "В данном случае можно даже не смотреть на предельный переход."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим лист дерева с числом элементов $N$ и target'ами $y_i$ в который попадает элемент с истинным таргетом $y_{ac}$ \n",
    "Оценим ошибку по MSE с ответом средними:\n",
    "\n",
    "$$Err_{mean} = (y_{ac} - \\frac{1}{N}\\sum_{k = 1}^N y_k)^2 = y_{ac}^2 - 2 y_{ac}y_{mean} + (y_{mean})^2$$\n",
    "\n",
    "И также ошибку с ответом случайным элементом $y_i$:\n",
    "$$Err_{rand} = (y_{ac} - y_{i})^2 = y_{ac}^2 - 2 y_{ac}y_{i} + y_{i}^2$$\n",
    "\n",
    "Найдём матожидание такой ошибки:\n",
    "$$E(Err_{rand}) = y_{ac}^2 - 2\\frac{1}{N} y_{ac}\\sum_{k = 1}^Ny_{k} + \\frac{1}{N}\\sum_{k = 1}^Ny_{k}^2 =\n",
    "y_{ac}^2 - 2 y_{ac}y_{mean} + (y^2)_{mean}$$\n",
    "\n",
    "Получается, что матожидание ошибки по MSE меньше на $(y^2)_{mean} -  (y_{mean})^2$, то есть на дисперсию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему такая стратегия не даёт ощутимого выигрыша?\n",
    "\n",
    "          Потому что мы уже сильно сузили область поиска, в этот лист попадают элементы с примерным значениям target'a равным среднему в листе. То есть и регрессию мы будем строить только по ним $\\rightarrow$ будем получать ответы с target'ом примерно равным среднему в листе.(Т.к. все таргеты примерно одинаковые в листе, то зависимость в линейной регресии от признаков будет очень слабая, то есть с маленькими кожфициентами).\n",
    "\n",
    "Как модифицировать?\n",
    "\n",
    "    Дерево должно быть достаточно низким, чтобы число признаков по которым производится отсев было меньше общего числа информативных признаков. Линейную регрессию в листе нужно строить именно по оставшимся признакам, не использоваными при построении пути к этому листу."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
